{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd33020e-66ee-43bd-a5ca-1c28e8fcdb3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent...\n",
      "Episode 100, Total Reward: -1, Exploration Rate: 0.38\n",
      "Episode 200, Total Reward: 0, Exploration Rate: 0.15\n",
      "Episode 300, Total Reward: 3, Exploration Rate: 0.06\n",
      "Episode 400, Total Reward: 2, Exploration Rate: 0.03\n",
      "Episode 500, Total Reward: 3, Exploration Rate: 0.02\n",
      "Episode 600, Total Reward: 3, Exploration Rate: 0.01\n",
      "Episode 700, Total Reward: 3, Exploration Rate: 0.01\n",
      "Episode 800, Total Reward: 3, Exploration Rate: 0.01\n",
      "Episode 900, Total Reward: 1, Exploration Rate: 0.01\n",
      "Episode 1000, Total Reward: 3, Exploration Rate: 0.01\n",
      "\n",
      "Testing the trained agent...\n",
      "Step 1: Ball Position: (0, 0), Action: down\n",
      "Step 2: Ball Position: (1, 0), Action: right\n",
      "Step 3: Ball Position: (1, 1), Action: right\n",
      "Step 4: Ball Position: (1, 2), Action: down\n",
      "Step 5: Ball Position: (2, 2), Action: right\n",
      "Step 6: Ball Position: (2, 3), Action: down\n",
      "Step 7: Ball Position: (3, 3), Action: down\n",
      "Step 8: Ball Position: (4, 3), Action: right\n",
      "Ball reached the pocket at (4, 4) in 8 steps!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the pool table environment\n",
    "class PoolTableEnv:\n",
    "    def __init__(self):\n",
    "        # Define the table dimensions (5x5 grid)\n",
    "        self.table_size = 5\n",
    "        # Define the pocket location (bottom-right corner)\n",
    "        self.pocket = (4, 4)\n",
    "        # Define the ball's starting position\n",
    "        self.ball_pos = (0, 0)\n",
    "        # Define possible actions (up, down, left, right)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        # Define rewards\n",
    "        self.reward_hit_pocket = 10\n",
    "        self.reward_miss = -1\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the ball to the starting position\n",
    "        self.ball_pos = (0, 0)\n",
    "        return self.ball_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move the ball based on the action\n",
    "        x, y = self.ball_pos\n",
    "        if action == 'up' and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 'down' and x < self.table_size - 1:\n",
    "            x += 1\n",
    "        elif action == 'left' and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 'right' and y < self.table_size - 1:\n",
    "            y += 1\n",
    "\n",
    "        self.ball_pos = (x, y)\n",
    "\n",
    "        # Check if the ball is in the pocket\n",
    "        if self.ball_pos == self.pocket:\n",
    "            reward = self.reward_hit_pocket\n",
    "            done = True\n",
    "        else:\n",
    "            reward = self.reward_miss\n",
    "            done = False\n",
    "\n",
    "        return self.ball_pos, reward, done\n",
    "\n",
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, max_exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.01):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.max_exploration_rate = max_exploration_rate\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.exploration_decay_rate = exploration_decay_rate\n",
    "\n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((env.table_size, env.table_size, len(env.actions)))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Exploration vs Exploitation\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            action = random.choice(self.env.actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            x, y = state\n",
    "            action_index = np.argmax(self.q_table[x, y])\n",
    "            action = self.env.actions[action_index]\n",
    "        return action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        action_index = self.env.actions.index(action)\n",
    "\n",
    "        # Q-learning formula\n",
    "        old_value = self.q_table[x, y, action_index]\n",
    "        next_max = np.max(self.q_table[next_x, next_y])\n",
    "        new_value = old_value + self.learning_rate * (reward + self.discount_factor * next_max - old_value)\n",
    "        self.q_table[x, y, action_index] = new_value\n",
    "\n",
    "    def decay_exploration_rate(self, episode):\n",
    "        # Decay exploration rate\n",
    "        self.exploration_rate = self.min_exploration_rate + (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate * episode)\n",
    "\n",
    "# Train the agent\n",
    "def train_agent(env, agent, num_episodes=1000):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Choose action\n",
    "            action = agent.choose_action(state)\n",
    "            # Take action and observe result\n",
    "            next_state, reward, done = env.step(action)\n",
    "            # Update Q-table\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Decay exploration rate\n",
    "        agent.decay_exploration_rate(episode)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate:.2f}\")\n",
    "\n",
    "# Test the trained agent\n",
    "def test_agent(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action (exploit only)\n",
    "        action = agent.choose_action(state)\n",
    "        print(f\"Step {steps + 1}: Ball Position: {state}, Action: {action}\")\n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    print(f\"Ball reached the pocket at {state} in {steps} steps!\")\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment and agent\n",
    "    env = PoolTableEnv()\n",
    "    agent = QLearningAgent(env)\n",
    "\n",
    "    # Train the agent\n",
    "    print(\"Training the agent...\")\n",
    "    train_agent(env, agent, num_episodes=1000)\n",
    "\n",
    "    # Test the trained agent\n",
    "    print(\"\\nTesting the trained agent...\")\n",
    "    test_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db12e0e-4417-4a89-950c-786824c56296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
